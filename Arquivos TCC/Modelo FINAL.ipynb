{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac39abb",
   "metadata": {},
   "source": [
    "# Versão final do modelo do TCC\n",
    "\n",
    "Lucca Cerf Costa | **RA00304770**   \n",
    "Letícia Lopes Bueno | **RA00297719**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877c88e",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import requests\n",
    "import warnings\n",
    "from folium.plugins import HeatMap\n",
    "import json\n",
    "\n",
    "from shapely.geometry import shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffb4a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Dados das Cidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe345d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo o documento que traz diversas informações sobre população, IDH, PIB per capita etc.\n",
    "dfc = pd.read_csv(r'DadosCidades/BRAZIL_CITIES_rev2022.csv')\n",
    "\n",
    "# Lista das colunas para manter\n",
    "manter = ['CITY','STATE', 'IBGE_RES_POP','IBGE_DU','IDHM','GVA_TOTAL','GDP','Cars','Motorcycles','POST_OFFICES']\n",
    "\n",
    "# Removendo todas as colunas que não estão na lista de colunas para manter\n",
    "remover = [coluna for coluna in dfc.columns if coluna not in manter]\n",
    "dfc = dfc.drop(columns=remover)\n",
    "dfc.head()\n",
    "\n",
    "# Precisamos transformar o IDH de cada municipio no IDH de cada estado\n",
    "# Faremos essa transformação por agregação ponderada\n",
    "# Primeiro, calculamos o produto entre a população residente (IBGE_RES_POP) e o IDHM de cada município\n",
    "dfc['Pop_IDHM'] = dfc['IBGE_RES_POP'] * dfc['IDHM']\n",
    "\n",
    "# Em seguida, agrupamos os dados por estado e calculamos a soma da população,a soma dos domicílios e \n",
    "# a soma dos produtos população-IDHM\n",
    "dfc2 = dfc.groupby('STATE').agg({\n",
    "    'IBGE_RES_POP': 'sum',\n",
    "    'IBGE_DU': 'sum',\n",
    "    'Pop_IDHM': 'sum',\n",
    "    'GVA_TOTAL': 'sum',\n",
    "    'GDP': 'sum',\n",
    "    'Cars': 'sum',\n",
    "    'Motorcycles': 'sum',\n",
    "    'POST_OFFICES': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculamos o IDH ponderado dividindo a soma dos produtos população-IDHM pela soma da população\n",
    "dfc2['IDH_Ponderado'] = dfc2['Pop_IDHM'] / dfc2['IBGE_RES_POP']\n",
    "\n",
    "# Descartamos a coluna do IDH-M\n",
    "dfc2.drop(columns=['Pop_IDHM'], inplace=True)\n",
    "\n",
    "# Renomeamos as colunas conforme necessário\n",
    "dfc2 = dfc2.rename(columns={\n",
    "    'STATE':'Estado',\n",
    "    'IBGE_RES_POP': 'População',\n",
    "    'IBGE_DU': 'Domicilios',\n",
    "    'GVA_TOTAL': 'GVA',\n",
    "    'GDP': 'PIB',\n",
    "    'Cars': 'Carros',\n",
    "    'Motorcycles': 'Motos',\n",
    "    'POST_OFFICES': 'Correios'\n",
    "})\n",
    "\n",
    "dfc2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798128d2",
   "metadata": {},
   "source": [
    "---\n",
    "## Dados de Alimentação e Produção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87141c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esse dataset traz a Segurança ALimentar de cada município e a quantidade de empresas de cada ramo de produção/alimentação\n",
    "dfs = pd.read_excel(r'Dados/DadosBR.xlsx').rename(columns={'Região':'Estado'})\n",
    "\n",
    "# Esses dados de Quantidade Produzida estão errados. Dados mais preciso estão no próximo dataframe\n",
    "dfs.drop(columns=['Quantidade Produzida (t)'], inplace=True)\n",
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4730b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esse dataset traz o número de animais em cada estado e o faturamento da agricultura em milhares de reais.\n",
    "dfp = pd.read_excel('Dados/ProdAlimentar.xlsx')\n",
    "dfp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed50aa7",
   "metadata": {},
   "source": [
    "---\n",
    "## Dados de Rodovias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9efc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esse dataset traz a porcentagem de estradas pavimentadas em cada estado\n",
    "dfr = pd.read_excel('DadosRodovias/df3_salvo.xlsx')\n",
    "\n",
    "# Calculando a porcentagem de pavimentação\n",
    "dfr['% pavimentada'] = ((dfr['pavimentada'] / (dfr['pavimentada'] + dfr['não-pavimentada'])) * 100).round(2)\n",
    "dfr.drop(columns=['pavimentada','não-pavimentada'], inplace=True)\n",
    "\n",
    "# Renomeando colunas para depois juntar os datasets baseando-se nos estados\n",
    "dfr.rename(columns={'Estados':'Estado'}, inplace=True)\n",
    "dfr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06079524",
   "metadata": {},
   "source": [
    "---\n",
    "## Dados EPSAN2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace78da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Montando um dataset baseado em diversas informações sobre alimentação. Todos os dados dos 4 datasets abaixo\n",
    "são oficiais do Mapeamento de Segurança Alimentar e Nutricional (MapaSAN). Outros estabelecimentos semelhantes podem\n",
    "existir mas não foram registrados até o momento de coleta dos dados (2018)'''\n",
    "\n",
    "# Banco de Alimentos\n",
    "dfBA = pd.read_csv(\"DadosEPSAN2018/Banco_Alimentos.csv\", encoding='latin1', delimiter = \";\")\n",
    "cont1 = dfBA['v9170'].value_counts().sort_index()\n",
    "\n",
    "# Cozinha Comunitária\n",
    "dfCC = pd.read_csv(\"DadosEPSAN2018/Cozinha_Comunitaria.csv\", encoding='latin1', delimiter = \";\")\n",
    "cont2 = dfCC['v9141'].value_counts().sort_index()\n",
    "\n",
    "# Restaurante Popular\n",
    "dfRP = pd.read_csv(\"DadosEPSAN2018/Restaurante_Popular.csv\", encoding='latin1', delimiter = \";\")\n",
    "cont3 = dfRP['v9109'].value_counts().sort_index()\n",
    "\n",
    "# Feira Livre\n",
    "dfFL = pd.read_csv(\"DadosEPSAN2018/Feira_Livre.csv\", encoding='latin1', delimiter = \";\")\n",
    "cont4 = dfFL['v9966'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737dd9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista com todos os estados do Brasil + DF\n",
    "estados = ['AC', 'AL', 'AM', 'AP', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', 'MG',\n",
    "           'MS', 'MT', 'PA', 'PB', 'PE', 'PI', 'PR', 'RJ', 'RN', 'RO', 'RR',\n",
    "           'RS', 'SC', 'SE', 'SP', 'TO']\n",
    "\n",
    "# Função para garantir que todos os estados estejam presentes\n",
    "def garantir_todos_estados(series, estados):\n",
    "    todos_estados_series = pd.Series(0, index=estados)\n",
    "    todos_estados_series.update(series)\n",
    "    return todos_estados_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garantindo que todas as variáveis tenham todos os estados\n",
    "ba = garantir_todos_estados(cont1, estados)\n",
    "cc = garantir_todos_estados(cont2, estados)\n",
    "rp = garantir_todos_estados(cont3, estados)\n",
    "fl = garantir_todos_estados(cont4, estados)\n",
    "\n",
    "dfe = pd.DataFrame({\n",
    "    'Estado': estados,\n",
    "    'Banco de Alimentos': ba,\n",
    "    'Cozinha Comunitária': cc,\n",
    "    'Restaurante Popular': rp,\n",
    "    'Feira Livre': fl\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "dfe.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c415801",
   "metadata": {},
   "source": [
    "### Dados dos hospitais e postos de saúde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b36d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh = pd.read_csv(r'DadosInfra/Hospital.csv')\n",
    "\n",
    "# Removendo colunas\n",
    "dfh.drop(dfh.columns[np.r_[2:17, 18:20, 22:31,32:37]], axis=1, inplace=True)\n",
    "\n",
    "# Renomeando colunas\n",
    "novos_nomes = {\n",
    "    'address': 'endereço',\n",
    "    'categories/0': 'categoria',\n",
    "    'city': 'cidade',\n",
    "    'location/lat': 'latitude',\n",
    "    'location/lng': 'longitude',\n",
    "    'state': 'estado'\n",
    "}\n",
    "\n",
    "dfh.rename(columns=novos_nomes, inplace=True)\n",
    "\n",
    "# Removendo valores não-relacionados\n",
    "valores_a_remover = ['Hospital equipment and supplies',\n",
    "                     'Veterinarian','Public university','Health insurance agency',\n",
    "                     'Association or organization','Blueprint service',\n",
    "                     'Transit station','Apartment building','Resort hotel']\n",
    "\n",
    "dfh = dfh[~dfh['categoria'].isin(valores_a_remover)]\n",
    "\n",
    "# Removendo NaN\n",
    "dfh = dfh.dropna(subset=['categoria'])\n",
    "dfh.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72768c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfps = pd.read_csv(r'DadosInfra/Posto de Saúde.csv')\n",
    "\n",
    "# Removendo colunas\n",
    "dfps.drop(dfps.columns[np.r_[1:11, 13:15, 17:26, 27:32]], axis=1, inplace=True)\n",
    "\n",
    "# Renomeando colunas\n",
    "novos_nomes = {\n",
    "    'address': 'endereço',\n",
    "    'categoryName': 'categoria',\n",
    "    'city': 'cidade',\n",
    "    'location/lat': 'latitude',\n",
    "    'location/lng': 'longitude',\n",
    "    'state' : 'estado'\n",
    "}\n",
    "\n",
    "dfps.rename(columns=novos_nomes, inplace=True)\n",
    "\n",
    "# Removendo valores não-relacionados\n",
    "valores_a_remover = ['Public educational institution','Compressed natural gas station','Garden','Perfume store',\n",
    "                     'Electric vehicle charging station','Condominium complex','Gated community',\n",
    "                     'Municipal office education','Health insurance agency','Lodging','Transportation service',\n",
    "                     'Museum','Shoe store','Public works department','State Dept of Culture','Telecommunications engineer',\n",
    "                     'Supermarket','Bus station','Park','State government office', 'Judaica store','Primary school',\n",
    "                     'Tailor','Wine store', 'Apartment building','Occupational medical physician',\n",
    "                     'Pharmaceutical products wholesaler', 'Housing complex', 'Public bathroom',\n",
    "                     'County government office', 'Junkyard','Housing development', 'Driving test center',\n",
    "                     'Local government office', 'Physician referral service', 'City district office','Pilates studio',\n",
    "                     'Non-profit organization','Education center', 'Dressmaker', 'School','Government office',\n",
    "                     'Public health department','Gas station','Government','Bus stop','Municipal Social Development',\n",
    "                     'Store','Pharmaceutical company','Parking lot','Nurse practitioner','Health and beauty shop',\n",
    "                     'College','Villa', 'City Hall','Elementary school','Post office','Association or organization',\n",
    "                     'Alternative medicine practitioner', 'City government office','State police', 'Cultural association',\n",
    "                     'Veterinary care', 'General practitioner','Utility contractor', 'ATM','Nursing school']\n",
    "\n",
    "dfps = dfps[~dfps['categoria'].isin(valores_a_remover)]\n",
    "\n",
    "# Removendo NaN\n",
    "dfps = dfps.dropna(subset=['categoria'])\n",
    "dfps.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c19bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juntando os dataframes de Hospitais e Postos de Saúde\n",
    "dfhps = pd.merge(dfh, dfps, how='outer')\n",
    "\n",
    "# Removendo linhas duplicadas com base nas colunas \"latitude\" e \"longitude\"\n",
    "dfhps = dfhps.drop_duplicates(subset=['latitude', 'longitude']).sort_values(by='cidade').dropna(subset=['latitude'])\n",
    "\n",
    "# Removendo linhas com valores nulos na coluna 'estado'\n",
    "dfhps = dfhps.dropna(subset=['estado'])\n",
    "\n",
    "# Função para substituir os nomes completos pelos códigos de estado\n",
    "def substituir_estado(nome_completo):\n",
    "    mapeamento_estados = {\n",
    "        'State of Acre': 'AC', 'State of Alagoas': 'AL', 'State of Amapá': 'AP',\n",
    "        'State of Amazonas': 'AM', 'State of Bahia': 'BA', 'Ceará': 'CE',\n",
    "        'Federal District': 'DF', 'State of Espírito Santo': 'ES', 'State of Goiás': 'GO',\n",
    "        'State of Maranhão': 'MA', 'State of Mato Grosso': 'MT', 'State of Mato Grosso do Sul': 'MS',\n",
    "        'State of Minas Gerais': 'MG', 'State of Pará': 'PA', 'State of Paraíba': 'PB',\n",
    "        'State of Paraná': 'PR', 'State of Pernambuco': 'PE', 'State of Piauí': 'PI',\n",
    "        'State of Rio de Janeiro': 'RJ', 'State of Rio Grande do Norte': 'RN',\n",
    "        'State of Rio Grande do Sul': 'RS', 'State of Rondônia': 'RO', 'State of Roraima': 'RR',\n",
    "        'State of Santa Catarina': 'SC', 'State of São Paulo': 'SP', 'State of Sergipe': 'SE',\n",
    "        'State of Tocantins': 'TO', 'Rondônia': 'RO', 'Pernambuco': 'PE',\n",
    "        'Santa Catarina': 'SC', 'Minas Gerais': 'MG', 'Goiás': 'GO'\n",
    "    }\n",
    "    if nome_completo in mapeamento_estados:\n",
    "        return mapeamento_estados[nome_completo]\n",
    "    elif isinstance(nome_completo, str):\n",
    "        for estado in mapeamento_estados:\n",
    "            if estado in nome_completo:\n",
    "                return mapeamento_estados[estado]\n",
    "    return nome_completo\n",
    "\n",
    "# Aplicando a função de substituição à coluna 'estado'\n",
    "dfhps['estado'] = dfhps['estado'].apply(substituir_estado)\n",
    "\n",
    "# Resetando o index\n",
    "dfhps.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dfhps.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhps.rename(columns={'estado': 'Estado'}, inplace=True)\n",
    "\n",
    "# Trocando \"Federal District\" por \"DF\" na coluna 'Estado'\n",
    "dfhps['Estado'] = dfhps['Estado'].replace('Federal District', 'DF')\n",
    "\n",
    "# Agrupando por estado\n",
    "dfhps2 = dfhps.groupby('Estado').size()\n",
    "\n",
    "# Convertendo o resultado para um DataFrame para melhor visualização\n",
    "dfhps2 = dfhps2.reset_index(name='Hospitais e Postos de Saúde')\n",
    "dfhps2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b09890",
   "metadata": {},
   "source": [
    "---\n",
    "## Dados Vigitel\n",
    "### Dados sobre obesidade, alimentação, uso de eletrônicos e fumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = pd.read_excel('DadosObesidade/DadosVigitel.xlsx')\n",
    "\n",
    "# Separando dados não normalizados para o mapa\n",
    "dv_mapa = pd.read_excel('DadosObesidade/DadosVigitel.xlsx')\n",
    "\n",
    "# Separando os dados de obesidade\n",
    "peso = dv[['Estado','% Sobrepeso', '% Obesidade']]\n",
    "\n",
    "# Removendo a porcentagem de obesidade e sobrepeso do modelo (serão o target)\n",
    "dv.drop(columns=['% Obesidade','% Sobrepeso'], inplace = True)\n",
    "dv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037761e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "peso.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9783a9",
   "metadata": {},
   "source": [
    "---\n",
    "### Juntando os datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8523e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a lista de DataFrames\n",
    "df_list = [dfc2, dfs, dfe, dfhps2, dfr, dfp, dv]\n",
    "\n",
    "# Mesclando os DataFrames\n",
    "df_final = df_list[0]  # Começa com o primeiro DataFrame da lista\n",
    "for df in df_list[1:]:\n",
    "    df_final = pd.merge(df_final, df, on='Estado', how='outer')\n",
    "    \n",
    "df_final['Pecuária(cabeças)']=df_final['Pecuária(cabeças)'].apply(lambda x: '{:.0f}'.format(x))\n",
    "df_final.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b1e9a5",
   "metadata": {},
   "source": [
    "---\n",
    "### Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_final\n",
    "\n",
    "# Ordenando os estados pela pontuação e colocando os dados sobre obesidade\n",
    "df = pd.merge(df, peso[['Estado',\n",
    "                        '% Sobrepeso',\n",
    "                        '% Obesidade']], on='Estado', how='right').sort_values(by='Estado', ascending=False)\n",
    "\n",
    "# Normalizando os dados (exceto Estado)\n",
    "scaler = MinMaxScaler()\n",
    "df[df.columns[1:]] = scaler.fit_transform(df[df.columns[1:]])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c35c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcorr = df.drop(columns=['Estado'])\n",
    "\n",
    "# Calculando a correlação com '% Sobrepeso' e '% Obesidade'\n",
    "correlation_sobrepeso = dfcorr.corrwith(dfcorr['% Sobrepeso'])\n",
    "correlation_obesidade = dfcorr.corrwith(dfcorr['% Obesidade'])\n",
    "\n",
    "# Exibindo as correlações\n",
    "print(\"Correlação com '% Sobrepeso':\")\n",
    "print(correlation_sobrepeso)\n",
    "print(\"\\nCorrelação com '% Obesidade':\")\n",
    "print(correlation_obesidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369764c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Excluir as variáveis % Sobrepeso e % Obesidade\n",
    "correlation_without_obesity = correlation_sobrepeso.drop(['% Sobrepeso', '% Obesidade'])\n",
    "\n",
    "# Separar as variáveis em grupos com base na correlação\n",
    "positivas = correlation_without_obesity[correlation_without_obesity > 0].nlargest(5)\n",
    "neutras = correlation_without_obesity[(correlation_without_obesity >= -0.1) & (correlation_without_obesity <= 0.1)].nlargest(3)\n",
    "negativas = correlation_without_obesity[correlation_without_obesity < 0].nsmallest(5)\n",
    "\n",
    "# Juntar os grupos\n",
    "variaveis_selecionadas = pd.concat([positivas, neutras, negativas])\n",
    "\n",
    "# Criar o gráfico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "variaveis_selecionadas.plot(kind='bar', color=['green' if x > 0 else 'red' for x in variaveis_selecionadas])\n",
    "plt.title('Correlação com % Sobrepeso')\n",
    "plt.xlabel('Variável')\n",
    "plt.ylabel('Correlação')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc690eb",
   "metadata": {},
   "source": [
    "---\n",
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d05527",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Definindo a coluna de target\n",
    "target = '% Obesidade'\n",
    "\n",
    "# Removendo a coluna 'Estado' para treinamento\n",
    "train_data = df.drop(columns=['Estado'])\n",
    "\n",
    "# Configurando o AutoGluon\n",
    "predictor = TabularPredictor(label=target, problem_type='regression').fit(train_data)\n",
    "\n",
    "# Importância das Features\n",
    "feature_importance = predictor.feature_importance(train_data)\n",
    "\n",
    "# Armazenando a importância das features em um DataFrame\n",
    "feature_importance_df = pd.DataFrame(feature_importance)\n",
    "\n",
    "# Avaliando o modelo\n",
    "performance = predictor.evaluate(train_data)\n",
    "\n",
    "# Armazenando as métricas de desempenho em um DataFrame\n",
    "performance_df = pd.DataFrame(performance, index=[0])\n",
    "\n",
    "# Obtendo o melhor modelo\n",
    "best_model = predictor.get_model_best()\n",
    "print(f\"Melhor modelo: {best_model}\")\n",
    "\n",
    "# Fazendo previsões no conjunto de treino\n",
    "predictions = predictor.predict(train_data.drop(columns=[target]))\n",
    "\n",
    "# Armazenando as previsões em um DataFrame\n",
    "predictions_df = train_data.copy()\n",
    "predictions_df[target + '_predicted'] = predictions\n",
    "\n",
    "# Avaliando novamente o modelo para verificar o desempenho\n",
    "performance_after_predictions = predictor.evaluate(train_data)\n",
    "performance_after_predictions_df = pd.DataFrame(performance_after_predictions, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f568e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c1def",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# y_true são os valores reais, y_pred são as previsões do modelo\n",
    "y_true = predictions_df['% Obesidade']\n",
    "y_pred = predictions_df['% Obesidade_predicted']\n",
    "\n",
    "# Calcular R²\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Converter R² para porcentagem\n",
    "accuracy = r2 * 100\n",
    "\n",
    "print(f\"A precisão do modelo é de {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab692783",
   "metadata": {},
   "source": [
    "---\n",
    "## Mapas\n",
    "### Mapa dos danos em rodovias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2d88a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Carregando o arquivo GeoJSON\n",
    "with open('brazil-states.geojson') as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "df_1 = pd.read_csv(r'C:\\Users\\lucca\\TCC\\DadosRodovias\\icm_08_2022.csv', delimiter=';')\n",
    "\n",
    "# Corrigindo o formato das coordenadas (substituir vírgulas por pontos)\n",
    "df_1['Longitude'] = df_1['Longitude'].str.replace(',', '.').astype(float)\n",
    "df_1['Latitude'] = df_1['Latitude'].str.replace(',', '.').astype(float)\n",
    "\n",
    "# Criando o mapa\n",
    "latitude_media = df_1['Latitude'].mean()\n",
    "longitude_media = df_1['Longitude'].mean()\n",
    "mapa = folium.Map(location=[latitude_media, longitude_media], zoom_start=5)\n",
    "\n",
    "# Adicionando marcadores simples para cada ponto\n",
    "for _, row in df_1.iterrows():\n",
    "    folium.Circle(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        radius=0.2,  # Tamanho do marcador\n",
    "        color='red',  # Cor do marcador\n",
    "        fill=True,\n",
    "        fill_color='red',\n",
    "        fill_opacity=0.3\n",
    "    ).add_to(mapa)\n",
    "\n",
    "# Adicionando o GeoJSON ao mapa\n",
    "folium.GeoJson(geojson_data).add_to(mapa)\n",
    "\n",
    "# Salvando o mapa como um arquivo HTML\n",
    "# mapa.save(\"mapa_icm_12_2022.html\")\n",
    "\n",
    "mapa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3fe57b",
   "metadata": {},
   "source": [
    "---\n",
    "### Mapa das estruturas de saúde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o mapa\n",
    "mapa = folium.Map(location=[-15.788, -47.879], zoom_start=4)\n",
    "\n",
    "# Convertendo as coordenadas (latitude e longitude) em uma lista de tuplas\n",
    "dados_calor = dfhps[['latitude', 'longitude']].values.tolist()\n",
    "\n",
    "# Criando o mapa de calor\n",
    "heat_map = HeatMap(dados_calor, radius=12, gradient={0.2: 'blue', 0.4: 'lime', 0.6: 'yellow', 0.8: 'orange', 1: 'red'})\n",
    "mapa.add_child(heat_map)\n",
    "\n",
    "mapa\n",
    "\n",
    "# Salvar o mapa como um arquivo HTML\n",
    "#mapa.save(\"mapa.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6940ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o mapa\n",
    "mapa = folium.Map(location=[-14.2350, -51.9253], zoom_start=4)\n",
    "\n",
    "# Adicionando choropleth\n",
    "folium.Choropleth(\n",
    "    geo_data=geojson_data,\n",
    "    data=dv_mapa,\n",
    "    columns=['Estado', '% Hipertensão'],\n",
    "    key_on='feature.properties.sigla',  # Usar a sigla do estado para correspondência\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='% Hipertensão'\n",
    ").add_to(mapa)\n",
    "\n",
    "# Função para obter o centro de um polígono\n",
    "def get_centroid(geojson_feature):\n",
    "    polygon = shape(geojson_feature['geometry'])\n",
    "    centroid = polygon.centroid\n",
    "    return centroid.y, centroid.x\n",
    "\n",
    "# Adicionando rótulos permanentes com base no centro dos estados\n",
    "for feature in geojson_data['features']:\n",
    "    coords = get_centroid(feature)\n",
    "    estado = feature['properties']['sigla']\n",
    "    folium.Marker(\n",
    "        location=coords,\n",
    "        icon=folium.DivIcon(html=f'<div style=\"font-size: 8pt; color: black;\">{estado}</div>')\n",
    "    ).add_to(mapa)\n",
    "\n",
    "# Salvar o mapa como um arquivo HTML\n",
    "#mapa.save(\"mapa_hipertensao.html\")\n",
    "\n",
    "mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf22d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('brazil-states.geojson') as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "mapa = folium.Map(location=[-14.2350, -51.9253], zoom_start=4)\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=geojson_data,\n",
    "    data=dv_mapa,\n",
    "    columns=['Estado', '% Diabetes'],\n",
    "    key_on='feature.properties.sigla',  # Usar a sigla do estado para correspondência\n",
    "    fill_color='BuPu',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='% Diabetes'\n",
    ").add_to(mapa)\n",
    "\n",
    "def get_centroid(geojson_feature):\n",
    "    polygon = shape(geojson_feature['geometry'])\n",
    "    centroid = polygon.centroid\n",
    "    return centroid.y, centroid.x\n",
    "\n",
    "for feature in geojson_data['features']:\n",
    "    coords = get_centroid(feature)\n",
    "    estado = feature['properties']['sigla']\n",
    "    folium.Marker(\n",
    "        location=coords,\n",
    "        icon=folium.DivIcon(html=f'<div style=\"font-size: 8pt; color: black;\">{estado}</div>')\n",
    "    ).add_to(mapa)\n",
    "\n",
    "# Salvar o mapa como um arquivo HTML\n",
    "#mapa.save(\"mapa_diabetes.html\")\n",
    "\n",
    "mapa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51f3821",
   "metadata": {},
   "source": [
    "---\n",
    "## Referências Bibliográficas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39386f0e",
   "metadata": {},
   "source": [
    "https://aplicacoes.mds.gov.br/sagi/mapasan/index.php?pasta=&el=%2F2018%2FBASE+DE+DADOS+E+RELAT%C3%93RIOS%2FNacional%2F4.+Epsans\n",
    "\n",
    "https://cidades.ibge.gov.br/\n",
    "\n",
    "https://www.kaggle.com/datasets/crisparada/brazilian-cities?resource=download\n",
    "\n",
    "https://www.mds.gov.br/webarquivos/cidadania/Caisan/MapaSan/MAPASAN%202018.pdf\n",
    "\n",
    "https://www.mds.gov.br/webarquivos/publicacao/seguranca_alimentar/mapa_san_resultados_preliminares.pdf\n",
    "\n",
    "https://dados.gov.br/home\n",
    "\n",
    "https://dados.gov.br/dados/conjuntos-dados/equipamentos-publicos-de-seguranca-alimentar-e-nutricional---2019\n",
    "\n",
    "https://dados.gov.br/dados/conjuntos-dados/condicoes-do-pavimento1\n",
    "\n",
    "https://pesquisarodovias.cnt.org.br/login?returnUrl=%2Fmapa\n",
    "\n",
    "https://apify.com/compass/crawler-google-places\n",
    "\n",
    "https://abeso.org.br/wp-content/uploads/2021/07/vigitel_brasil_2019_vigilancia_fatores_risco-1-2.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
